## Intro

Здесь мы изложим подход для детекции или предсказания событий во временных рядах. Наш подход будет основываться на скрытых марковских цепях и реккурентных нейронных сетях. Затем укажем на достоинства и недостатки этого метода, предложим улучшение или другие методы решения задачи.

## Hidden Markov Chain

Скрытая марковские цепь -- a statistical Markov model in which the system being modeled is assumed to be a Markov process — with unobservable ("hidden") states. Скрытые состояния не видны наблюдателю, а распределение наблюдения зависит от текущего скрытого состояния. Не смотря на скрытость состояний, мы можем определить наиболее вероятное, исходя из наблюдений. Также данную модель можно использовать для предсказания будущих состояний.

Для нашей задачи события, которые мы хотим обнаруживать, - это скрытые состояния марковской модели.

## Подход

### Разметка

Перед тем, как приступить к построению модели и ее обучению, необходимо разметить данные. Для разметки данных можно использовать алгоритмы или людей.

### Набор преобразоваиий и факторов

В качестве входных данных для модели можно использовать различные преобразования нашего временного ряда, а также другие временные ряды. Это возжомно благодаря тому, что модели можно подавать вектора на вход, рассматривая их как случайные величины, зависящие от скрытых состояний. Данный этап очень важен, так как от выбора преобразования зависит эффективность алгоритма. Например, применить различные фильтры для выделения тренда, сезонности, шума или иных особенностей, сглаживания, дифферентиаторы. Чем больше, тем лучше, потому что, прогнав получившиеся временные ряды через модель и сравнивая результаты с ground-truth разметкой, можно отобрать наиболее полезные признаки. Это довольно долгий процесс, так как различных признаков может набраться не одна сотня.

### Первичное обучение 

Для первого приближения можно использовать гаусовские смеси в качестве распределений наблюдений. Для обучения нужны три алгоритма:
- Алгоритм прямого-обратного хода: даны параметры модели и последовательность, требуется вычислить вероятность появления данной последовательности (позволяет решить задачу).
- Алгоритм Витерби: даны параметры модели, требуется определить наиболее подходящую последовательность скрытых узлов, наиболее точно описывающую данную модель (помогает при решении данной задачи).
-  Алгоритм Баума — Велша: дана выходная последовательность (или несколько) с дискретными значениями, требуется обучить СММ на данном выходе.

### Дальнейшее обучение

Теперь, для более лучшего приближения распределений наблюдений можно заменить гаусовские смеси на градиентный бустинг или иной алгоритм машинного обучения.

### Завершающий этап

Для устранения недостатков модели мы конкатенируем выходы со входами и подаем в LSTM.

## Преемущества

- многомерность: можно обучить на нескольких временных рядах (факторах) одновременно;
- позволяет отранжировать факторы по степени полезности;
- unsupervised (метки нужны только для выбора фактора)
- есть реализации в библиотеках

## Недостатки

- марковость (т.е. отсутствие памяти)
- БЕЗ LSTM наблюдается некоторый лаг (после посещения локального максимума, все еще не меняет состояние). Вероятно, причина этому - марковость hmm. Именно поэтому и добавляется LSTM.
- Зависит от выбора факторов, но которых обучается. То бишь, препроцессинг играет большую роль.
- Зависит от выбора likelihood-распределений;
- Отсутствие изменчивости и адаптивности к текущей ситуации. Эта проблема скорее всего решается изменением процесса обучения;
- Вероятно нужна модификация для появления плавности в смене состояний и наблюдений; 

## Улучшения

- для избавления от Марковости можно заменить на hmm высокого порядка, которая позволяет сохранять информацию на некоторое заданное число шагов
- заменить на байесовскую сеть
- заменить на Partially observable Markov decision process. Это позволит использовать всю мощь методов renforcement learning, сохраняя при этом свойства hmm;

## Другие алгоритмы
- Transformers могут также подойти и для временного ряда;
- gan + convolution;



# Keywords to search for
- trend following indicator
- trend filter
- trend prediction
- market regime prediction / detection  
- regime shift

# Math Frameworks

- Hidden Markov Model
- Kalman Filter 
  иногда это частный случай hidden markov model
- Partially observable Markov decision process  
  для Reinforcement Learning
- Dynamic Bayesian Network  
  generalization of hidden markov models and kalman filter

# Literature

## LSTM + HMM

<https://arxiv.org/pdf/2104.09700.pdf>

### Что сделали

1. Разметка с использованием метода tripple barier(нужна для выбора фактора);
2. Из большого числа различных факторов выбрали наилучшие,
   обучив на них gmm-hmm и используя метки из прошлого пункта <https://arxiv.org/pdf/2104.09700.pdf#table.caption.5>;
3. Обучили на модели из предыдущего пункта xgb-hmm;
4. Прогнали модели из предыдущего пункта на нескольких факторах и выход пустили в lstm;

### Advantages

- многомерность: можно обучить на нескольких временных рядах (факторах) одновременно;
- реакция на изменение не только тренда, но и волатильности;
- в зависимости от препроцессинга может научиться детектить и предиктить не только тренды;
- позволяет отранжировать факторы по степени полезности;

### Disadvantages

- БЕЗ LSTM наблюдается некоторый лаг (после посещения локального максимума, все еще не меняет состояние). Вероятно, причина этому - марковость hmm. Именно поэтому и добавляется LSTM.
- Зависит от выбора факторов, на которых обучается. Препроцессинг играет большую роль.
- Зависит от выбора likelihood-распределений;

## Старые алгоритмы

<https://www.sciencedirect.com/science/article/pii/S0169207099000485>

### Что сделали

Еще в двухтысячных привели список алгоритмов.

## High Order HMM

<https://www.sciencedirect.com/science/article/pii/S0378437118314018>

### Что сделали

Обучили high order hmm c гаусом. Преобразование: normalized log return. Описали, каким именно образом использовать выход hmm для предсказаний.

### Advantages

Смогли избавиться от марковости, заменив ее на марковость длины (n, m).

# URLs

- [О hmm и их использовании в трейдинге](https://www.quantstart.com/articles/hidden-markov-models-an-introduction/)
- [Статьи по тегу trend-following](https://alphaarchitect.com/category/architect-academic-insights/trend-following/)
- Цикл статей:
- <https://alphaarchitect.com/2020/08/an-introduction-to-digital-signal-processing-for-trend-following/>
- <https://alphaarchitect.com/2020/12/trend-following-filters-part-1-2/>
- <https://alphaarchitect.com/2021/01/trend-following-filters-part-2-2/>
- <https://alphaarchitect.com/2021/04/trend-following-filters-part-3/>
- <https://alphaarchitect.com/2022/01/trend-following-filters-part-4/>
- <https://alphaarchitect.com/2020/06/30/time-series-momentum-theory-and-evidence/>
- [trend following indicators](https://finance.yahoo.com/news/complete-guide-trend-following-indicators-100425674.html)
- [kalman filter](https://bookdown.org/rdpeng/timeseriesbook/general-kalman-filter.html)
